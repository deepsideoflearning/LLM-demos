{"cells":[{"cell_type":"markdown","id":"a5d4d722","metadata":{"id":"a5d4d722"},"source":["## LangChain basics\n","#### So let's demonstrate some of LangChain's capabilities. Let's pip install it along the OpenAI python package:"]},{"cell_type":"code","execution_count":null,"id":"2735d443","metadata":{"id":"2735d443"},"outputs":[],"source":["#!pip install langchain openai"]},{"cell_type":"markdown","id":"ee511fb8","metadata":{"id":"ee511fb8"},"source":["We set up the OpenAI API key as part of the environment variables.\n","(https://platform.openai.com/api-keys)"]},{"cell_type":"code","execution_count":1,"id":"9f508ab5","metadata":{"id":"9f508ab5","executionInfo":{"status":"ok","timestamp":1701193306472,"user_tz":300,"elapsed":377,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}}},"outputs":[],"source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = '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'"]},{"cell_type":"markdown","id":"6b6604ba","metadata":{"id":"6b6604ba"},"source":["### LLMs\n","#### We can import the OpenAI’s GPT-3 model and ask it questions"]},{"cell_type":"code","execution_count":2,"id":"416b10b7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"416b10b7","executionInfo":{"status":"ok","timestamp":1701193350410,"user_tz":300,"elapsed":18534,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}},"outputId":"8fa80d88-8e51-48b8-e60c-ffdfd77ed68f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter question: Who is the president of the Philippines?\n","\n","\n","The current president of the Philippines is Rodrigo Duterte.\n"]}],"source":["from langchain.llms import OpenAI\n","\n","llm = OpenAI()\n","\n","question = input('Enter question: ')\n","print(llm.predict(question))"]},{"cell_type":"markdown","id":"32ccfe7b","metadata":{"id":"32ccfe7b"},"source":["#### We can also import the underlying model behind ChatGPT"]},{"cell_type":"code","execution_count":3,"id":"b5b708cf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"b5b708cf","executionInfo":{"status":"ok","timestamp":1701193427667,"user_tz":300,"elapsed":15304,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}},"outputId":"a882c99a-5d89-454e-b79c-89dba12b8065"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter question: Who is the president of the Philippines?\n"]},{"output_type":"execute_result","data":{"text/plain":["'As of September 2021, the president of the Philippines is Rodrigo Duterte.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["from langchain.chat_models import ChatOpenAI\n","\n","chat_model = ChatOpenAI()\n","\n","question = input('Enter question: ')\n","chat_model.predict(question)"]},{"cell_type":"markdown","id":"73358fb9","metadata":{"id":"73358fb9"},"source":["#### The problem with raw LLMs is that they don’t remember the history of the conversations."]},{"cell_type":"code","execution_count":4,"id":"ae24bfd4","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ae24bfd4","executionInfo":{"status":"ok","timestamp":1701193463116,"user_tz":300,"elapsed":3192,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}},"outputId":"4052fa6c-53e3-4bb9-f731-4e118323f340"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I'm sorry, but as an AI language model, I don't have the capability to recall your previous question.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["chat_model.predict('What was my previous question?')"]},{"cell_type":"markdown","id":"36fe45ec","metadata":{"id":"36fe45ec"},"source":["### Chains\n","#### We can create a chain that is going to help us augment the LLM"]},{"cell_type":"code","execution_count":null,"id":"0bb48231","metadata":{"id":"0bb48231"},"outputs":[],"source":["from langchain.chains import ConversationChain\n","\n","chain = ConversationChain(\n","    llm = chat_model,\n","    verbose=False\n",")"]},{"cell_type":"code","execution_count":null,"id":"5090269c","metadata":{"id":"5090269c"},"outputs":[],"source":["while True:\n","\n","    question = input('Enter question: ')\n","\n","    if question == 'q':\n","        print(\"good bye!\")\n","        break\n","\n","#    clear_output(wait=True)\n","    print(chain.run(question))"]},{"cell_type":"markdown","id":"0a7a4e1c","metadata":{"id":"0a7a4e1c"},"source":["## Prompt templates\n","#### With LangChain, we can automate a lot of the prompt engineering, and for that, we can use prompt templates. Let’s create a prompt template"]},{"cell_type":"code","execution_count":null,"id":"5e7bd8bf","metadata":{"id":"5e7bd8bf"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"\n","Return all the subcategories of the following category\n","\n","{category}\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=['category'],\n","    template=template\n",")"]},{"cell_type":"markdown","id":"ce6f8dc2","metadata":{"id":"ce6f8dc2"},"source":["#### ‘Category’ is an input variable that will be used once we run the chain. Let’s input it into a chain. We use LLMChain, which is the simplest chain we can use"]},{"cell_type":"code","execution_count":null,"id":"08b36778","metadata":{"id":"08b36778"},"outputs":[],"source":["from langchain.chains import LLMChain\n","\n","chain = LLMChain(\n","    llm=chat_model,\n","    prompt=prompt,\n","    verbose=True\n",")\n","\n","chain.run('Machine Learning')"]},{"cell_type":"code","execution_count":null,"id":"2b5a4e98","metadata":{"id":"2b5a4e98"},"outputs":[],"source":["print(chat_model.predict('Return all the subcategories of the following category \"Machine Learning\"'))"]},{"cell_type":"markdown","id":"cc18b435","metadata":{"id":"cc18b435"},"source":["#### We can also break down the prompts into the system and human prompts. This is helpful when we build chatbots"]},{"cell_type":"code","execution_count":null,"id":"c062ccce","metadata":{"id":"c062ccce"},"outputs":[],"source":["from langchain.prompts import (\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate,\n","    ChatPromptTemplate\n",")\n","\n","system_template = \"\"\"\n","You are a helpful assistant who generate comma separated lists.\n","A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n","ONLY return comma separated and nothing more!\n","\"\"\"\n","\n","human_template = '{category}'\n","\n","system_message = SystemMessagePromptTemplate.from_template(\n","    system_template\n",")\n","\n","human_message = HumanMessagePromptTemplate.from_template(\n","    human_template\n",")"]},{"cell_type":"markdown","id":"2102c8a6","metadata":{"id":"2102c8a6"},"source":["#### And we can combine the 2 prompts into one:"]},{"cell_type":"code","execution_count":null,"id":"e83ddca3","metadata":{"id":"e83ddca3"},"outputs":[],"source":["prompt = ChatPromptTemplate.from_messages([\n","    system_message, human_message\n","])\n","\n","chain = LLMChain(\n","    llm=chat_model,\n","    prompt=prompt,\n","    verbose=True\n",")\n","\n","\n","chain.run('Machine Learning')"]},{"cell_type":"code","execution_count":null,"id":"411eac04","metadata":{"id":"411eac04"},"outputs":[],"source":["print(chat_model.predict('''\n","You are a helpful assistant who generate comma separated lists.\n","A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n","ONLY return comma separated and nothing more!\n","\n","Machine Learning\n","'''))"]},{"cell_type":"markdown","id":"23c9e99c","metadata":{"id":"23c9e99c"},"source":["#### This gives us more control of the LLM’s output, but we can go further by using an output parser."]},{"cell_type":"markdown","id":"92775345","metadata":{"id":"92775345"},"source":["## Output parser\n","#### Let’s overwrite the base output parser and generate Python lists from the LLM’s response:"]},{"cell_type":"code","execution_count":null,"id":"9b471165","metadata":{"scrolled":true,"id":"9b471165"},"outputs":[],"source":["from langchain.schema import BaseOutputParser\n","\n","class CommaSeparatedParser(BaseOutputParser):\n","    def parse(self, text):\n","        output = text.strip().split(',')\n","        output = [o.strip() for o in output]\n","        return output\n","\n","chain = LLMChain(\n","    llm=chat_model,\n","    prompt=prompt,\n","    output_parser=CommaSeparatedParser(),\n","    verbose=True\n",")\n","\n","chain.run('Machine Learning')"]},{"cell_type":"markdown","id":"3fd74169","metadata":{"id":"3fd74169"},"source":["#### We can also feed the chain with multiple inputs:"]},{"cell_type":"code","execution_count":null,"id":"ae658c0e","metadata":{"id":"ae658c0e"},"outputs":[],"source":["input_list = [\n","    {'category': 'food'},\n","    {'category': 'country'},\n","    {'category': 'colors'}\n","]\n","\n","response = chain.apply(input_list)"]},{"cell_type":"code","execution_count":null,"id":"ec3e7b2a","metadata":{"id":"ec3e7b2a"},"outputs":[],"source":["response[1]['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PoBzMi2vV4c"},"outputs":[],"source":["response[2]['text']"],"id":"3PoBzMi2vV4c"},{"cell_type":"markdown","id":"0bc9cf49","metadata":{"id":"0bc9cf49"},"source":["## Simple Sequence\n","#### We can also compose chains. Here we are creating a pipeline of chains, where the output of one chain is used as input in the next one. We create 2 chains: a title chain and a synopsis to automate the process of writing a play. First, we have a title chain:"]},{"cell_type":"code","execution_count":null,"id":"5a7fe278","metadata":{"id":"5a7fe278"},"outputs":[],"source":["title_template = \"\"\"\n","You are a writer. Given a subject,\n","your job is to return a fun title for a play.\n","\n","Subject: {subject}\n","Title:\"\"\"\n","\n","title_chain = LLMChain.from_string(\n","    llm=chat_model,\n","    template=title_template\n",")\n","\n","title_chain.run('Machine Learning')"]},{"cell_type":"markdown","id":"36f1b7b8","metadata":{"id":"36f1b7b8"},"source":["#### And a synopsis chain:"]},{"cell_type":"code","execution_count":null,"id":"ceb18fa4","metadata":{"id":"ceb18fa4"},"outputs":[],"source":["synopsis_template = \"\"\"\n","You are a writer.\n","Given a title, write a synopsis for a play.\n","\n","Title: {title}\n","Synopsis:\"\"\"\n","\n","synopsis_chain = LLMChain.from_string(\n","    llm=chat_model,\n","    template=synopsis_template\n",")\n","\n","title = \"The Algorithmic Adventure: A Machine Learning Marvel\"\n","\n","synopsis_chain.run(title)"]},{"cell_type":"markdown","id":"ab20beb7","metadata":{"id":"ab20beb7"},"source":["#### Let’s now combine those 2 chains by passing them as a list to the simple sequential chain:"]},{"cell_type":"code","execution_count":null,"id":"a9c8904d","metadata":{"id":"a9c8904d"},"outputs":[],"source":["from langchain.chains import SimpleSequentialChain\n","\n","chain = SimpleSequentialChain(\n","    chains=[title_chain, synopsis_chain],\n","    verbose=True\n",")\n","\n","chain.run('Machine Learning')"]},{"cell_type":"code","execution_count":null,"id":"7ca0266a","metadata":{"id":"7ca0266a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}