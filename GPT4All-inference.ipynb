{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/jonkrohn/NLP-with-LLMs/blob/main/code/GPT4All-inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# GPT4All CPU Interface\n","\n","In this notebook, we jump as quickly as we can into a command-line interaction with a GPT-4-like model that is on our own device."],"metadata":{"id":"Powa-rm3ApS0"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"vVpLf3_fX3qK","executionInfo":{"status":"ok","timestamp":1701059764689,"user_tz":300,"elapsed":1117,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}}},"outputs":[],"source":["%%capture\n","!pip install nomic"]},{"cell_type":"code","source":["from nomic.gpt4all import GPT4AllGPU"],"metadata":{"id":"eegz7O7Nuvj0","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"error","timestamp":1701059971069,"user_tz":300,"elapsed":193,"user":{"displayName":"Christopher Himmel","userId":"17951251819139000091"}},"outputId":"f9baebc0-632a-431e-b980-853bf5d20285"},"execution_count":8,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-85b3c00f6964>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnomic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt4all\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT4AllGPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nomic.gpt4all'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["m = GPT4All()\n","m.open()"],"metadata":{"id":"XZ3Rz0grAneG","outputId":"ce4cfa9f-eb98-43d0-86f4-7f5931e4b944","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m2023-05-04 07:48:01.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.gpt4all.gpt4all\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m77\u001b[0m - \u001b[1mDownloading executable...\u001b[0m\n","51KB [00:00, 709.18KB/s]              \n","\u001b[32m2023-05-04 07:48:01.606\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnomic.gpt4all.gpt4all\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m80\u001b[0m - \u001b[1mDownloading model...\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["File downloaded successfully to /root/.nomic/gpt4all\n"]},{"output_type":"stream","name":"stderr","text":["514250KB [01:37, 5274.58KB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["File downloaded successfully to /root/.nomic/gpt4all-lora-quantized.bin\n"]}]},{"cell_type":"code","source":["m.prompt('Please answer this question: What is a Large Language Model?')"],"metadata":{"id":"H_Fscz0qBAAl","outputId":"9c0408e2-27ad-4de0-c15a-2894e4d6364d","colab":{"base_uri":"https://localhost:8080/","height":124}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1 A large language model (LLM) refers to an artificial intelligence algorithm that learns from vast amounts of data and can be trained on multiple domains, including natural language processing tasks like machine translation or text classification/retrieval. LLMs are typically based on deep learning techniques such as recurrent neural networks (RNN), which allow them to learn complex patterns in unstructured datasets without being explicitly programmed for each task they perform.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]}]}